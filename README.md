# MoEE Router Workbench — README & Research Assessment

> Clarity-aware routing and ambiguity-aware damping for Mixture/Orchestrator systems. This repo provides a **router workbench** (telemetry + controllers) and a **research study** validating a key empirical result: **clear prompts contract, ambiguous prompts expand**, mediated by **gating sensitivity**.

---

## Table of Contents

* [Overview](#overview)
* [Key Claims](#key-claims)
* [Architecture](#architecture)
* [Quick Start](#quick-start)
* [API](#api)
* [Telemetry (Shadow Console)](#telemetry-shadow-console)
* [Controllers](#controllers)
* [Research Assessment](#research-assessment)

  * [Hypotheses](#hypotheses)
  * [Metrics](#metrics)
  * [Experiment Design](#experiment-design)
  * [Results](#results)
  * [Mechanism (Mediation)](#mechanism-mediation)
  * [Threats to Validity](#threats-to-validity)
* [Reproducing the Study](#reproducing-the-study)
* [Dashboards](#dashboards)
* [Roadmap](#roadmap)
* [Contributing](#contributing)
* [License](#license)

---

## Overview

**MoEE Router Workbench** is a measurement-first framework for routing and orchestration around LLM experts. It ships:

* A **Shadow Console** to log ambiguity ($A^*$), routing distributions, gating sensitivity, and outcomes.
* **Controllers** that sharpen routing for clear inputs and damp thrash for ambiguous ones.
* **Study code** to evaluate contraction/expansion behavior and verify causal mediation through the gate.

This repo is intentionally model-agnostic (works with any model that yields per-expert logits or mixture weights) and focuses on **telemetry-driven design** rather than speculative theory.

---

## Key Claims

1. **Clarity → contraction; Ambiguity → expansion.** Measured orthogonal Lipschitz $L_\perp$ is **lower** (more contractive) on clear prompts and **higher** on ambiguous prompts.
2. **Mechanism:** The effect is **mediated by gating sensitivity** $G = \|\Delta w\| / \|\Delta x\|$. Ambiguity increases $G$, and higher $G$ increases $L_\perp$.
3. **Control:** A clarity-aware controller (β, EMA-λ schedules) **reduces thrash and jitter** in high-ambiguity regions without harming accuracy on clear tasks.

---

## Architecture

```
client → router (logits z_t) → softmax(β·z_t) → w_t
        ↘ Shadow Console (A*, H_route, ∥∇w∥, G, L⊥, S, latency, success)
        ↘ Controller (β(A*), λ(A*)) → EMA of routes: ŵ_t = norm((1-λ)·ŵ_{t-1} + λ·w_t)
        → expert mixture f(x) = Σ ŵ_{t,i} f_i(x)
```

**Core components**

* `router/` — produces logits per expert.
* `controller/` — clarity-aware schedules for β (decisiveness) and λ (routing inertia).
* `telemetry/` — event schema + writers (Parquet/JSONL) for Shadow Console.
* `study/` — experiment harness & analysis notebooks.

---

## Quick Start

1. **Bring up services** (example docker-compose layout):

```bash
cd deployment/docker
docker-compose up -d
```

2. **Run the usage example:**

```bash
python3 examples/basic_usage.py
```

3. **Enable controllers:** set `CONTROLLER_POLICY=ctrl_v1` in env or config.

> *Note:* This workbench is model-agnostic. Provide either **expert logits** (`z_t`) or directly the **route distribution** `w_t`. See the API below.

---

## API

### Route Endpoint (example)

**POST** `/route`

```json
{
  "logits": [0.5, -0.2, 0.8, -0.5, 0.3],
  "ambiguity_score": 0.80,
  "request_id": "abc-123"
}
```

**Response**

```json
{
  "weights": [0.23, 0.08, 0.46, 0.05, 0.18],
  "beta": 1.20,
  "lambda": 0.55,
  "policy_id": "ctrl_v1"
}
```

**Notes**

* If you send `weights` instead of `logits`, the server skips the softmax step.
* When `policy_id=ctrl_v1`, the server will apply the clarity-aware β/λ schedules.

---

## Telemetry (Shadow Console)

Each request emits an event like:

```json
{
  "ts": 1725040000,
  "request_id": "r_…",
  "user_id": "u_…",
  "template_id": "tpl_…",
  "task_type": "reasoning|code|gen|…",
  "model_id": "qwen2.5-7b",
  "controller": {"policy_id": "base|ctrl_v1", "beta": 1.20, "lambda": 0.55, "lr_S": 0.001},
  "metrics": {
    "A_star": 0.31,
    "H_route": 0.42,
    "d_w_norm": 0.18,        
    "G": 0.62,               
    "L_perp": 0.93,
    "L_para": 1.47,
    "self_sim": 0.78,
    "R_PR": 142.3,
    "R_H": 128.6
  },
  "io": {"prompt_len": 182, "tokens_out": 236, "latency_ms": 812},
  "labels": {"judge_score": 0.86, "success": true},
  "versions": {"router_git": "…", "s_matrix_git": "…"}
}
```

**Key signals**

* $A^*$: composite ambiguity (predictive flatness, paraphrase disagreement, routing indecision, spec errors).
* $G$: gating sensitivity, finite difference on routes (fixed $\Delta x$).
* $L_\perp$: orthogonal Lipschitz (median across K random orthogonal directions).
* `H_route`: routing entropy; useful but inferior to **distance-to-vertex** $D = 1-\max_i w_i$ in some regimes.

---

## Controllers

Let **clarity** $C = 1 - A^*$.

* **β schedule (decisiveness)**: `β = β_min + (β_max − β_min) · C`
  e.g., `β_min=0.8`, `β_max=1.6`.
* **EMA schedule (inertia)**: `λ = λ_min + (λ_max − λ_min) · C`
  e.g., `λ_min=0.2`, `λ_max=0.8`.
* **Cooldown**: if $\|\nabla w\|$ spikes 3 ticks, hold β,λ for 5 ticks.
* **S-matrix LR** (optional): `η_S = η_0 · (1 + k · C)`; freeze updates when `A* > 0.7` unless judge improves.

**Goal**: be decisive when the ask is clear, and damp thrash when the ask is ambiguous.

---

## Research Assessment

### Hypotheses

* **H0 (rejected):** Ambiguity contracts the system ($L_\perp\downarrow$).
* **H1 (supported):** Ambiguity expands ($L_\perp\uparrow$), clarity contracts.
* **Mediation:** $A^* \to G \to L_\perp$ carries most of the effect.

### Metrics

* **Ambiguity ($A^*$)**: composite in $[0,1]$ combining predictive flatness (token entropy), paraphrase disagreement (embedding variance + JSD over routes), routing indecision (entropy, effective experts), and spec incompleteness/conflict.
* **Self-similarity (S)**: cosine agreement across reruns/perturbations.
* **Lipschitz** $L_\perp$: response change $\|\Delta y\|/\|\Delta x\|$ along orthogonal semantic directions (median of K=4–8).
* **Gating sensitivity (G)**: $\|\Delta w\|/\|\Delta x\|$ with fixed $\Delta x$.
* **Effective rank**: spectral-entropy rank $R_H$ and participation ratio $R_{PR}$ (windowed, mean-centered, unit-normed).

### Experiment Design

* **Binning**: 5 bins for $A^*$ across $[0,1]$, \~10 samples per bin (N≈50).
* **Stratification**: analyze top tertile of routing entropy and distance-to-vertex.
* **Robustness**: winsorize $L_\parallel$ per template, evaluate $L_\perp$; bootstrap CIs; Theil–Sen slopes per template.
* **Ablations**: router frozen (w fixed), ghost-gate (reuse w across perturbed inputs), β/λ off vs on.

### Results

* **Correlation (clean set, N≈32)**: $\rho(A^*,L_\perp)$ weak **positive**; per-bin $P(L_\perp<1)$ higher for low $A^*$.
  *Interpretation*: clarity contracts, ambiguity expands.
* **Mediator evidence**: preliminary plot shows **$\rho(A^*,G)\approx 0.46$** across sparse samples; follow-up analyses recommend confirming **$\rho(G,L_\perp)>0$** and partial $\rho(A^*,L_\perp\,|\,G)\to 0$.
* **Rank**: near-full effective rank in long windows; local compression emerges when slicing by $A^*$ and shrinking windows (32–64 ticks).

### Mechanism (Mediation)

Mixture sensitivity decomposes as
$\|J_f\| \lesssim \sum_i w_i\,\|J_{f_i}\| + \Big\|\sum_i f_i(x)\, \nabla w_i(x)^\top\Big\|$
Ambiguity widens $w$ and increases $\nabla w$, raising the gating term → **expansion**. Clarity peaks $w$, shrinking $\nabla w$ → **contraction**.

### Threats to Validity

* **Range compression**: too-narrow $A^*$ span → noisy ρ. Addressed with Sobol sampling across knobs.
* **Coupled axes**: measuring $L$ along the same paraphrase axis used for $A^*$ can bias signs. We split $L_\parallel$ vs $L_\perp$.
* **Non-stationarity**: online S-matrix updates can drift; use frozen-router slices for attribution.
* **Artifacts**: effective-rank inflation without mean-centering/unit-norming; frozen-vector bugs (now fixed).

---

## Reproducing the Study

1. **Collect telemetry** with the Shadow Console enabled.
2. **Run the evaluator** (binning + bootstrap + early-stop):

   * Script: `study/ambiguity_contraction_eval.py` (see repo; includes per-bin summaries, stratified Spearman with 95% CI, Theil–Sen slopes).
3. **Mediation test**:

   * Compute ρ(A\*,G), ρ(G, L⊥), and partial ρ(A\*, L⊥ | G) via rank-residuals.
4. **Controller A/B**:

   * Re-run 2–3 with `policy_id=base` vs `ctrl_v1`; compare jitter, thrash time, and $P(L_\perp<1)$ by $A^*$ bin.

*Expected signatures*

* With controller **on**: $G\downarrow$, $L_\perp\downarrow$ in high-$A^*$ bins; negligible change in clear bins; latency jitter ↓.

---

## Dashboards

* **Contraction Map**: heatmap of $A^*$ bins × $P(L_\perp<1)$, split by policy.
* **Thrash Map**: task\_type × %time($\|\nabla w\|>\tau$); curvature overlays.
* **Mediator Panel**: scatter G vs $L_\perp$ with trend; partial correlations side panel.
* **Utilization**: expert firing rates by task; entropy drift monitor.

---

## Roadmap

* ✅ Establish telemetry schema & evaluators
* ✅ Validate sign: clarity contracts, ambiguity expands
* ⏳ Full mediation proof and CI across models (Qwen ↔ Llama)
* ⏳ Counterfactual router replay at scale (offline A/B)
* ⏳ Per-slice controllers (code vs reasoning vs gen)
* ⏳ Data curation for targeted MoE fine-tuning (Phase 2)

---

## Contributing

PRs welcome. Please:

* Add unit tests for new metrics/controllers.
* Include a sample event file (JSONL) when changing telemetry schema.
* Provide before/after plots or table summaries for research claims.

---

## License

Specify your license (e.g., MIT) in `LICENSE`.
