#!/usr/bin/env python3
"""
Enhanced Mediator Proof - Lock Down the A* ‚Üí G ‚Üí L_‚ä• Causal Chain
Implements exact partial Spearman via rank-residuals as specified
Key test: œÅ(A*, L_‚ä• | G) ‚Üí 0 confirms G mediates the relationship
"""

import numpy as np
import json
import matplotlib.pyplot as plt
from scipy.stats import rankdata, linregress, spearmanr, pearsonr
from pathlib import Path
import requests
from typing import List, Dict, Tuple, Optional
from sklearn.linear_model import LinearRegression
import warnings
warnings.filterwarnings('ignore')

def get_embedding(text: str) -> Optional[np.ndarray]:
    """Get embedding from Ollama"""
    try:
        response = requests.post(
            'http://localhost:11434/api/embeddings',
            json={'model': 'all-minilm', 'prompt': text},
            timeout=5
        )
        if response.status_code == 200:
            return np.array(response.json()['embedding'])
    except:
        pass
    return None

def partial_spearman(x: np.ndarray, y: np.ndarray, z: np.ndarray) -> Tuple[float, float]:
    """
    Partial Spearman via rank-residuals (exact implementation)
    
    Args:
        x, y, z: Input arrays
        
    Returns:
        rho: Partial correlation œÅ(x,y|z)
        p: p-value
    """
    # Rank-transform all variables
    xr = rankdata(x).astype(float)
    yr = rankdata(y).astype(float) 
    zr = rankdata(z).astype(float)
    
    # Regress out z from x and y, get residuals
    bx = linregress(zr, xr)
    rx = xr - (bx.slope * zr + bx.intercept)
    
    by = linregress(zr, yr)
    ry = yr - (by.slope * zr + by.intercept)
    
    # Spearman on residuals
    rho, p = spearmanr(rx, ry)
    
    return rho, p

def bootstrap_partial_correlation(x: np.ndarray, y: np.ndarray, z: np.ndarray, 
                                 n_bootstrap: int = 1000) -> Dict:
    """Bootstrap confidence intervals for partial correlation"""
    n = len(x)
    bootstrap_rhos = []
    
    for _ in range(n_bootstrap):
        # Bootstrap sample
        indices = np.random.choice(n, n, replace=True)
        x_boot = x[indices]
        y_boot = y[indices]
        z_boot = z[indices]
        
        # Compute partial correlation
        rho_boot, _ = partial_spearman(x_boot, y_boot, z_boot)
        if not np.isnan(rho_boot):
            bootstrap_rhos.append(rho_boot)
    
    if len(bootstrap_rhos) < 10:
        return {'mean': 0.0, 'ci_low': 0.0, 'ci_high': 0.0, 'valid': False}
    
    bootstrap_rhos = np.array(bootstrap_rhos)
    
    return {
        'mean': np.mean(bootstrap_rhos),
        'ci_low': np.percentile(bootstrap_rhos, 2.5),
        'ci_high': np.percentile(bootstrap_rhos, 97.5),
        'std': np.std(bootstrap_rhos),
        'valid': True
    }

def compute_distance_to_vertex(weights: np.ndarray) -> float:
    """
    Compute distance-to-vertex D = 1 - max_i w_i
    Often tracks L_‚ä• better than routing entropy
    """
    return 1.0 - np.max(weights)

def winsorize_array(arr: np.ndarray, limits: Tuple[float, float] = (0.05, 0.05)) -> np.ndarray:
    """Winsorize array at specified percentiles"""
    lower_limit = np.percentile(arr, limits[0] * 100)
    upper_limit = np.percentile(arr, (1 - limits[1]) * 100)
    return np.clip(arr, lower_limit, upper_limit)

def compute_gating_weights_stable(embeddings: List[np.ndarray], temperature: float = 1.0) -> np.ndarray:
    """Compute gating weights with stable normalization"""
    if len(embeddings) < 2:
        return np.array([1.0])
    
    n_experts = min(5, len(embeddings))
    prototypes = embeddings[:n_experts]
    current = embeddings[-1]
    
    # Unit-norm + mean-center as specified
    current = current / (np.linalg.norm(current) + 1e-9)
    current = current - np.mean(current)
    current = current / (np.linalg.norm(current) + 1e-9)
    
    logits = []
    for proto in prototypes:
        proto = proto / (np.linalg.norm(proto) + 1e-9)
        proto = proto - np.mean(proto)
        proto = proto / (np.linalg.norm(proto) + 1e-9)
        
        similarity = np.dot(current, proto)
        logits.append(similarity)
    
    logits = np.array(logits) * temperature
    logits = logits - np.max(logits)  # Stability
    exp_logits = np.exp(logits)
    weights = exp_logits / (np.sum(exp_logits) + 1e-9)
    
    return weights

def compute_enhanced_gating_sensitivity(prompt1: str, prompt2: str, 
                                       delta_x_fixed: float = 0.01) -> Optional[Dict]:
    """
    Enhanced gating sensitivity with fixed Œîx as specified
    G = ||Œîw||/||Œîx|| where Œîx is kept constant
    """
    emb1 = get_embedding(prompt1)
    emb2 = get_embedding(prompt2)
    
    if emb1 is None or emb2 is None:
        return None
    
    # Normalize and mean-center
    emb1 = emb1 / (np.linalg.norm(emb1) + 1e-9)
    emb2 = emb2 / (np.linalg.norm(emb2) + 1e-9)
    
    # Scale to fixed delta_x
    direction = emb2 - emb1
    direction = direction / (np.linalg.norm(direction) + 1e-9)
    emb2_scaled = emb1 + direction * delta_x_fixed
    
    # Context embeddings for prototypes
    context_prompts = [
        "The answer is", "This means that", "In other words", 
        "The result is", "Therefore"
    ]
    
    context_embeddings = []
    for cp in context_prompts:
        cemb = get_embedding(cp)
        if cemb is not None:
            context_embeddings.append(cemb)
    
    if len(context_embeddings) < 3:
        return None
    
    # Compute gating weights
    embeddings1 = context_embeddings + [emb1]
    embeddings2 = context_embeddings + [emb2_scaled]
    
    w1 = compute_gating_weights_stable(embeddings1)
    w2 = compute_gating_weights_stable(embeddings2)
    
    # Compute metrics
    delta_w = np.linalg.norm(w2 - w1)
    G = delta_w / delta_x_fixed  # Fixed denominator as specified
    
    # Distance-to-vertex (often better than entropy)
    D1 = compute_distance_to_vertex(w1)
    D2 = compute_distance_to_vertex(w2)
    delta_D = abs(D2 - D1)
    
    return {
        'G': float(G),
        'delta_x': float(delta_x_fixed),
        'delta_w': float(delta_w),
        'w1': w1.tolist(),
        'w2': w2.tolist(),
        'D1': float(D1),
        'D2': float(D2),
        'delta_D': float(delta_D),
        'w1_entropy': float(-np.sum(w1 * np.log(w1 + 1e-9))),
        'w2_entropy': float(-np.sum(w2 * np.log(w2 + 1e-9)))
    }

def compute_orthogonal_lipschitz(prompt1: str, prompt2: str, 
                                K: int = 4) -> Optional[Dict]:
    """
    Compute L_‚ä• using orthogonal semantic directions (median of K)
    Separate from L_parallel to avoid coupling bias
    """
    emb1 = get_embedding(prompt1)
    emb2 = get_embedding(prompt2)
    
    if emb1 is None or emb2 is None:
        return None
    
    # Normalize
    emb1 = emb1 / (np.linalg.norm(emb1) + 1e-9)
    emb2 = emb2 / (np.linalg.norm(emb2) + 1e-9)
    
    # Direction of change (for L_parallel)
    main_direction = emb2 - emb1
    main_direction = main_direction / (np.linalg.norm(main_direction) + 1e-9)
    
    # Generate K random orthogonal directions
    lipschitz_values = []
    np.random.seed(42)  # Reproducible
    
    for k in range(K):
        # Random direction
        random_dir = np.random.randn(*emb1.shape)
        
        # Make orthogonal to main direction
        orthogonal_dir = random_dir - np.dot(random_dir, main_direction) * main_direction
        orthogonal_dir = orthogonal_dir / (np.linalg.norm(orthogonal_dir) + 1e-9)
        
        # Perturb in orthogonal direction
        perturb_scale = 0.01
        emb1_perturb = emb1 + orthogonal_dir * perturb_scale
        emb2_perturb = emb2 + orthogonal_dir * perturb_scale
        
        # Simulate responses (in production, use actual model)
        np.random.seed(hash(prompt1 + prompt2 + str(k)) % 2**32)
        resp1 = emb1_perturb + np.random.randn(*emb1.shape) * 0.1
        resp2 = emb2_perturb + np.random.randn(*emb2.shape) * 0.1
        
        # Lipschitz in this direction
        delta_y = np.linalg.norm(resp2 - resp1)
        delta_x = np.linalg.norm(emb2_perturb - emb1_perturb)
        L_k = delta_y / (delta_x + 1e-9)
        
        lipschitz_values.append(L_k)
    
    # Use median as specified (more robust than mean)
    L_perp = np.median(lipschitz_values)
    
    # Also compute L_parallel for comparison
    delta_x_parallel = np.linalg.norm(emb2 - emb1)
    np.random.seed(hash(prompt1 + prompt2) % 2**32)
    resp1_par = emb1 + np.random.randn(*emb1.shape) * 0.1
    resp2_par = emb2 + np.random.randn(*emb2.shape) * 0.1
    delta_y_parallel = np.linalg.norm(resp2_par - resp1_par)
    L_parallel = delta_y_parallel / (delta_x_parallel + 1e-9)
    
    return {
        'L_perp': float(L_perp),
        'L_parallel': float(L_parallel),
        'L_values': [float(x) for x in lipschitz_values],
        'L_perp_std': float(np.std(lipschitz_values)),
        'delta_x_parallel': float(delta_x_parallel),
        'delta_y_parallel': float(delta_y_parallel)
    }

def estimate_ambiguity_enhanced(prompt: str) -> float:
    """Enhanced ambiguity estimation A*"""
    prompt_lower = prompt.lower()
    
    # Question indicators 
    question_words = ['what', 'how', 'why', 'when', 'where', 'which', 'explain', 'describe']
    question_score = sum(1 for w in question_words if w in prompt_lower) / len(question_words)
    
    # Abstract/philosophical concepts
    abstract_words = ['meaning', 'purpose', 'consciousness', 'intelligence', 'emergence', 
                     'recursive', 'boundary', 'nature', 'reality', 'truth', 'existence']
    abstract_score = sum(1 for w in abstract_words if w in prompt_lower) / len(abstract_words)
    
    # Definite/computational indicators (lower ambiguity)
    definite_words = ['calculate', 'compute', 'add', 'subtract', 'multiply', 'divide', 
                     'is', 'are', 'equals', 'result']
    definite_score = sum(1 for w in definite_words if w in prompt_lower) / len(definite_words)
    
    # Uncertainty markers (higher ambiguity)
    uncertainty_words = ['maybe', 'might', 'could', 'perhaps', 'unclear', 'ambiguous', 'complex']
    uncertainty_score = sum(1 for w in uncertainty_words if w in prompt_lower) / len(uncertainty_words)
    
    # Combined A* score
    A_star = (0.3 * question_score + 
              0.3 * abstract_score + 
              0.2 * uncertainty_score + 
              0.2 * (1 - definite_score))
    
    return np.clip(A_star, 0, 1)

def run_enhanced_mediator_proof():
    """
    Main enhanced mediator test - lock down A* ‚Üí G ‚Üí L_‚ä• causal chain
    """
    print("=" * 70)
    print("ENHANCED MEDIATOR PROOF - LOCK DOWN THE CAUSAL CHAIN")
    print("Testing: œÅ(A*, L_‚ä• | G) ‚Üí 0 confirms G mediates")
    print("=" * 70)
    
    # Enhanced test pairs with broader ambiguity range
    test_pairs = [
        # Very low ambiguity (computational)
        ("What is 2+2?", "What is 3+3?"),
        ("Calculate 15 * 4", "Calculate 16 * 5"),
        ("The capital of France is Paris", "The capital of Italy is Rome"),
        
        # Low ambiguity (factual)
        ("How many days in a year?", "How many hours in a day?"),
        ("What color is the sky?", "What color is grass?"),
        
        # Medium-low ambiguity
        ("Explain photosynthesis briefly", "Explain respiration briefly"),
        ("How do cars work?", "How do planes work?"),
        
        # Medium ambiguity  
        ("What causes happiness?", "What causes sadness?"),
        ("Why do people dream?", "Why do people forget?"),
        
        # Medium-high ambiguity
        ("What is consciousness?", "What is intelligence?"),
        ("Explain the nature of time", "Explain the nature of space"),
        
        # High ambiguity (philosophical)
        ("What is the meaning of existence?", "What is the purpose of reality?"),
        ("What emerges from complexity?", "What emerges from chaos?"),
        ("The boundary between meaning and void", "The boundary between truth and illusion"),
        ("Recursive self-reference creates", "Recursive meta-cognition enables"),
        
        # Very high ambiguity (abstract/incomplete)
        ("When the observer becomes", "When the system transcends"),
        ("In the space between thoughts", "In the gap between moments")
    ]
    
    print(f"Testing {len(test_pairs)} prompt pairs across ambiguity spectrum...")
    
    results = []
    failed_pairs = []
    
    for i, (prompt1, prompt2) in enumerate(test_pairs):
        print(f"\nPair {i+1}: Processing...")
        print(f"  P1: {prompt1[:60]}...")
        print(f"  P2: {prompt2[:60]}...")
        
        # Compute enhanced ambiguity
        A1 = estimate_ambiguity_enhanced(prompt1)
        A2 = estimate_ambiguity_enhanced(prompt2)
        A_star = (A1 + A2) / 2
        
        # Compute enhanced gating sensitivity (fixed Œîx)
        gating_result = compute_enhanced_gating_sensitivity(prompt1, prompt2)
        
        # Compute orthogonal Lipschitz
        lipschitz_result = compute_orthogonal_lipschitz(prompt1, prompt2)
        
        if gating_result is not None and lipschitz_result is not None:
            G = gating_result['G']
            L_perp = lipschitz_result['L_perp']
            L_parallel = lipschitz_result['L_parallel']
            D1 = gating_result['D1']
            D2 = gating_result['D2']
            
            print(f"  A* = {A_star:.3f}")
            print(f"  G = {G:.4f} (gating sensitivity)")
            print(f"  L_‚ä• = {L_perp:.3f} (orthogonal Lipschitz)")  
            print(f"  L_‚à• = {L_parallel:.3f} (parallel, for reference)")
            print(f"  D = {D1:.3f} ‚Üí {D2:.3f} (distance-to-vertex)")
            
            results.append({
                'pair_id': i + 1,
                'prompt1': prompt1,
                'prompt2': prompt2,
                'A_star': A_star,
                'G': G,
                'L_perp': L_perp,
                'L_parallel': L_parallel,
                'D1': D1,
                'D2': D2,
                'delta_D': gating_result['delta_D'],
                'delta_w': gating_result['delta_w'],
                'delta_x': gating_result['delta_x']
            })
        else:
            print(f"  Failed (embedding/computation error)")
            failed_pairs.append(i + 1)
    
    if len(results) < 5:
        print(f"\nInsufficient data: {len(results)} valid pairs (need ‚â•5)")
        return
    
    print(f"\n{'='*70}")
    print(f"CORRELATION ANALYSIS - {len(results)} valid pairs")
    print(f"{'='*70}")
    
    # Convert to arrays
    A_values = np.array([r['A_star'] for r in results])
    G_values = np.array([r['G'] for r in results])
    L_perp_values = np.array([r['L_perp'] for r in results])
    L_parallel_values = np.array([r['L_parallel'] for r in results])
    D_values = np.array([r['delta_D'] for r in results])
    
    # Winsorize L_parallel as specified
    L_parallel_winsorized = winsorize_array(L_parallel_values, (0.05, 0.05))
    
    # Direct correlations
    rho_AL_perp, p_AL_perp = spearmanr(A_values, L_perp_values)
    rho_AG, p_AG = spearmanr(A_values, G_values)
    rho_GL_perp, p_GL_perp = spearmanr(G_values, L_perp_values)
    rho_AD, p_AD = spearmanr(A_values, D_values)  # Distance-to-vertex
    
    print(f"Direct correlations:")
    print(f"  œÅ(A*, L_‚ä•) = {rho_AL_perp:+.3f} (p={p_AL_perp:.3f})")
    print(f"  œÅ(A*, G)   = {rho_AG:+.3f} (p={p_AG:.3f}) ‚Üê KEY: Should be positive")
    print(f"  œÅ(G, L_‚ä•)  = {rho_GL_perp:+.3f} (p={p_GL_perp:.3f}) ‚Üê KEY: Should be positive") 
    print(f"  œÅ(A*, ŒîD)  = {rho_AD:+.3f} (p={p_AD:.3f}) (distance-to-vertex)")
    
    # THE CRITICAL TEST: Partial correlation œÅ(A*, L_‚ä• | G)
    print(f"\nüéØ CRITICAL MEDIATION TEST:")
    rho_AL_given_G, p_AL_given_G = partial_spearman(A_values, L_perp_values, G_values)
    print(f"  œÅ(A*, L_‚ä• | G) = {rho_AL_given_G:+.3f} (p={p_AL_given_G:.3f})")
    
    # Bootstrap confidence intervals
    print(f"\nüìä BOOTSTRAP ANALYSIS (1000 iterations):")
    partial_bootstrap = bootstrap_partial_correlation(A_values, L_perp_values, G_values)
    if partial_bootstrap['valid']:
        print(f"  œÅ(A*, L_‚ä• | G) = {partial_bootstrap['mean']:+.3f} "
              f"[{partial_bootstrap['ci_low']:+.3f}, {partial_bootstrap['ci_high']:+.3f}] 95% CI")
    
    # MEDIATION INTERPRETATION
    print(f"\n{'='*70}")
    print(f"MEDIATION VERDICT")
    print(f"{'='*70}")
    
    mediation_confirmed = (abs(rho_AL_given_G) < 0.2 and 
                          abs(rho_AL_perp) > 0.3 and 
                          rho_AG > 0.2 and 
                          rho_GL_perp > 0.2)
    
    mediation_ratio = abs(rho_AL_given_G) / (abs(rho_AL_perp) + 1e-9)
    
    if mediation_confirmed:
        print("‚úÖ MEDIATION CONFIRMED - G IS THE CAUSAL PATHWAY!")
        print(f"  ‚Ä¢ Strong A* ‚Üí G: œÅ = {rho_AG:+.3f}")
        print(f"  ‚Ä¢ Strong G ‚Üí L_‚ä•: œÅ = {rho_GL_perp:+.3f}")
        print(f"  ‚Ä¢ Direct A* ‚Üí L_‚ä•: œÅ = {rho_AL_perp:+.3f}")
        print(f"  ‚Ä¢ Partial A* ‚Üí L_‚ä•|G: œÅ = {rho_AL_given_G:+.3f} ‚Üê COLLAPSED!")
        print(f"  ‚Ä¢ Mediation ratio: {mediation_ratio:.2f} (< 0.5 = strong mediation)")
        print(f"\nüéØ ACTIONABLE: Reduce G in high-A* regions ‚Üí caps L_‚ä•")
        
    elif abs(rho_AG) > 0.3:
        print("‚ö†Ô∏è  PARTIAL MEDIATION - G is important but not complete")
        print(f"  ‚Ä¢ A* drives G: œÅ = {rho_AG:+.3f}")
        print(f"  ‚Ä¢ But partial correlation still significant: œÅ = {rho_AL_given_G:+.3f}")
        print(f"  ‚Ä¢ Other pathways may exist beyond gating sensitivity")
        
    else:
        print("‚ùå MEDIATION NOT ESTABLISHED")
        print(f"  ‚Ä¢ Weak A* ‚Üí G relationship: œÅ = {rho_AG:+.3f}")
        print(f"  ‚Ä¢ Need stronger evidence that ambiguity drives gating volatility")
    
    # Comparison with distance-to-vertex
    print(f"\nüìè DISTANCE-TO-VERTEX COMPARISON:")
    print(f"  œÅ(A*, ŒîD) = {rho_AD:+.3f} vs œÅ(A*, L_‚ä•) = {rho_AL_perp:+.3f}")
    if abs(rho_AD) > abs(rho_AL_perp):
        print("  ‚Üí Distance-to-vertex tracks ambiguity better than L_‚ä•")
    
    # CONTROL POLICY IMPLICATIONS
    if mediation_confirmed or abs(rho_AG) > 0.3:
        print(f"\nüéõÔ∏è  CONTROL POLICY RECOMMENDATIONS:")
        print(f"  ‚Ä¢ Œ≤ schedule: Lower when A* high ‚Üí reduces G")
        print(f"  ‚Ä¢ Œª schedule: More inertia when A* high ‚Üí dampens gate thrash")
        print(f"  ‚Ä¢ Target: Keep G < 2.0 in high-A* regions")
        print(f"  ‚Ä¢ Expected outcome: L_‚ä• ‚Üì while maintaining decisiveness on clear tasks")
    
    # Save comprehensive results
    output_path = Path("validation/results/enhanced_mediator_results.json")
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w') as f:
        json.dump({
            'summary': {
                'n_pairs_tested': len(test_pairs),
                'n_valid_results': len(results),
                'failed_pairs': failed_pairs,
                'mediation_confirmed': mediation_confirmed,
                'mediation_ratio': float(mediation_ratio)
            },
            'correlations': {
                'rho_A_L_perp': float(rho_AL_perp),
                'p_A_L_perp': float(p_AL_perp),
                'rho_A_G': float(rho_AG),
                'p_A_G': float(p_AG),
                'rho_G_L_perp': float(rho_GL_perp),
                'p_G_L_perp': float(p_GL_perp),
                'rho_A_L_given_G': float(rho_AL_given_G),
                'p_A_L_given_G': float(p_AL_given_G),
                'rho_A_D': float(rho_AD),
                'p_A_D': float(p_AD)
            },
            'bootstrap': partial_bootstrap,
            'detailed_results': results
        }, f, indent=2)
    
    print(f"\nüíæ Results saved to {output_path}")
    
    # Enhanced visualization
    create_enhanced_mediation_plots(A_values, G_values, L_perp_values, L_parallel_winsorized, 
                                   D_values, rho_AG, rho_GL_perp, rho_AL_perp, rho_AL_given_G)
    
    return results

def create_enhanced_mediation_plots(A_values, G_values, L_perp_values, L_parallel_values,
                                   D_values, rho_AG, rho_GL_perp, rho_AL_perp, rho_AL_given_G):
    """Create comprehensive mediation visualization"""
    try:
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # A* vs G (KEY: should be positive)
        axes[0,0].scatter(A_values, G_values, alpha=0.7, s=60, color='orange')
        axes[0,0].set_xlabel('Ambiguity (A*)')
        axes[0,0].set_ylabel('Gating Sensitivity (G)')
        axes[0,0].set_title(f'A* ‚Üí G: œÅ={rho_AG:.3f}\n(KEY: Positive confirms ambiguity drives gating)')
        axes[0,0].grid(True, alpha=0.3)
        
        # Add trend line
        z = np.polyfit(A_values, G_values, 1)
        p_line = np.poly1d(z)
        x_range = np.linspace(A_values.min(), A_values.max(), 100)
        axes[0,0].plot(x_range, p_line(x_range), 'r--', alpha=0.7)
        
        # G vs L_‚ä• (KEY: should be positive)  
        axes[0,1].scatter(G_values, L_perp_values, alpha=0.7, s=60, color='green')
        axes[0,1].set_xlabel('Gating Sensitivity (G)')
        axes[0,1].set_ylabel('Orthogonal Lipschitz (L_‚ä•)')
        axes[0,1].set_title(f'G ‚Üí L_‚ä•: œÅ={rho_GL_perp:.3f}\n(KEY: Positive confirms gating drives expansion)')
        axes[0,1].grid(True, alpha=0.3)
        
        z2 = np.polyfit(G_values, L_perp_values, 1)
        p_line2 = np.poly1d(z2)
        x_range2 = np.linspace(G_values.min(), G_values.max(), 100)
        axes[0,1].plot(x_range2, p_line2(x_range2), 'r--', alpha=0.7)
        
        # A* vs L_‚ä• (Direct relationship)
        axes[0,2].scatter(A_values, L_perp_values, alpha=0.7, s=60, color='blue')
        axes[0,2].set_xlabel('Ambiguity (A*)')
        axes[0,2].set_ylabel('Orthogonal Lipschitz (L_‚ä•)')
        axes[0,2].set_title(f'A* ‚Üí L_‚ä• (Direct): œÅ={rho_AL_perp:.3f}')
        axes[0,2].grid(True, alpha=0.3)
        
        # Partial correlation visualization (residuals)
        reg_AG = LinearRegression().fit(A_values.reshape(-1, 1), G_values)
        reg_LG = LinearRegression().fit(G_values.reshape(-1, 1), L_perp_values)
        
        A_residual = A_values - reg_AG.predict(A_values.reshape(-1, 1)) 
        L_residual = L_perp_values - reg_LG.predict(G_values.reshape(-1, 1))
        
        axes[1,0].scatter(A_residual, L_residual, alpha=0.7, s=60, color='red')
        axes[1,0].set_xlabel('A* (residual after removing G effect)')
        axes[1,0].set_ylabel('L_‚ä• (residual after removing G effect)')
        axes[1,0].set_title(f'CRITICAL: œÅ(A*,L_‚ä•|G)={rho_AL_given_G:.3f}\n(Should ‚Üí 0 if G mediates)')
        axes[1,0].grid(True, alpha=0.3)
        axes[1,0].axhline(y=0, color='k', linestyle='--', alpha=0.5)
        axes[1,0].axvline(x=0, color='k', linestyle='--', alpha=0.5)
        
        # Distance-to-vertex comparison
        axes[1,1].scatter(A_values, D_values, alpha=0.7, s=60, color='purple')
        axes[1,1].set_xlabel('Ambiguity (A*)')
        axes[1,1].set_ylabel('Œî Distance-to-Vertex')
        axes[1,1].set_title('A* vs Distance-to-Vertex Change\n(Alternative to routing entropy)')
        axes[1,1].grid(True, alpha=0.3)
        
        # L_‚ä• vs L_‚à• comparison (should be different)
        axes[1,2].scatter(L_parallel_values, L_perp_values, alpha=0.7, s=60, color='brown')
        axes[1,2].set_xlabel('L_‚à• (Parallel, Winsorized)')
        axes[1,2].set_ylabel('L_‚ä• (Orthogonal)')
        axes[1,2].set_title('L_‚à• vs L_‚ä•\n(Orthogonal avoids coupling bias)')
        axes[1,2].grid(True, alpha=0.3)
        axes[1,2].plot([0, max(L_parallel_values)], [0, max(L_parallel_values)], 'k--', alpha=0.5)
        
        plt.tight_layout()
        
        plot_path = Path("validation/results/enhanced_mediation_analysis.png")
        plot_path.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        print(f"üé® Enhanced mediation plots saved to {plot_path}")
        plt.close()
        
    except Exception as e:
        print(f"Plotting failed: {e}")

if __name__ == '__main__':
    print("üöÄ Starting Enhanced Mediator Proof...")
    print("This will test if gating sensitivity G mediates A* ‚Üí L_‚ä•")
    print("Expected: œÅ(A*, G) > 0.3, œÅ(G, L_‚ä•) > 0.3, œÅ(A*, L_‚ä• | G) ‚Üí 0")
    print("-" * 70)
    
    results = run_enhanced_mediator_proof()
    
    if results and len(results) >= 5:
        print("\nüéØ SUMMARY:")
        print(f"‚Ä¢ Tested {len(results)} prompt pairs")
        print(f"‚Ä¢ Use results to calibrate Œ≤/Œª schedules in controller")
        print(f"‚Ä¢ Target: Keep G < 2.0 in high-ambiguity regions")
        print(f"‚Ä¢ Expected outcome: L_‚ä• reduction without harming clear-task performance")
    else:
        print("\n‚ö†Ô∏è  Need more valid samples to establish mediation")
        print("Check Ollama service or expand test set")