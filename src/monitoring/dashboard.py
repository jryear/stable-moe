#!/usr/bin/env python3
"""
MoE Routing Stability Dashboard
Real-time monitoring for 4.72x stability improvement
"""

import streamlit as st
import requests
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import time
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import numpy as np

# Configure page
st.set_page_config(
    page_title="MoE Routing Stability Dashboard",
    page_icon="üéØ",
    layout="wide",
    initial_sidebar_state="expanded"
)

class DashboardConfig:
    """Dashboard configuration"""
    API_BASE_URL = "http://localhost:8000"
    REFRESH_INTERVAL = 5  # seconds
    METRICS_HISTORY_LIMIT = 1000
    
    # Thresholds
    GATING_SENSITIVITY_THRESHOLD = 0.1
    WINNER_FLIP_THRESHOLD = 0.05
    LATENCY_THRESHOLD_MS = 100
    CLARITY_THRESHOLD = 0.7

@st.cache_data(ttl=5)
def fetch_health_status() -> Optional[Dict]:
    """Fetch controller health status"""
    try:
        response = requests.get(f"{DashboardConfig.API_BASE_URL}/health", timeout=2)
        if response.status_code == 200:
            return response.json()
    except requests.exceptions.RequestException:
        pass
    return None

@st.cache_data(ttl=5)
def fetch_metrics() -> Optional[Dict]:
    """Fetch current controller metrics"""
    try:
        response = requests.get(f"{DashboardConfig.API_BASE_URL}/metrics", timeout=2)
        if response.status_code == 200:
            return response.json()
    except requests.exceptions.RequestException:
        pass
    return None

@st.cache_data(ttl=10)
def fetch_recent_metrics(limit: int = 100) -> Optional[Dict]:
    """Fetch recent routing metrics"""
    try:
        response = requests.get(
            f"{DashboardConfig.API_BASE_URL}/recent-metrics", 
            params={"limit": limit},
            timeout=5
        )
        if response.status_code == 200:
            return response.json()
    except requests.exceptions.RequestException:
        pass
    return None

def validate_improvement() -> Optional[Dict]:
    """Trigger controller validation test"""
    try:
        response = requests.post(f"{DashboardConfig.API_BASE_URL}/validate", timeout=10)
        if response.status_code == 200:
            return response.json()
    except requests.exceptions.RequestException:
        pass
    return None

def create_status_indicator(status: str, improvement_factor: str) -> None:
    """Create status indicator widget"""
    if status == "healthy":
        st.success(f"üü¢ Controller Status: HEALTHY ({improvement_factor})")
    elif status == "degraded":
        st.warning(f"üü° Controller Status: DEGRADED ({improvement_factor})")
    else:
        st.error(f"üî¥ Controller Status: UNHEALTHY ({improvement_factor})")

def create_metrics_cards(metrics: Dict) -> None:
    """Create metric summary cards"""
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        requests_processed = metrics.get('requests_processed', 0)
        st.metric(
            label="Requests Processed",
            value=f"{requests_processed:,}",
            delta=None
        )
    
    with col2:
        avg_gating_sensitivity = metrics.get('avg_gating_sensitivity', 0)
        delta_color = "inverse" if avg_gating_sensitivity > DashboardConfig.GATING_SENSITIVITY_THRESHOLD else "normal"
        st.metric(
            label="Avg Gating Sensitivity", 
            value=f"{avg_gating_sensitivity:.4f}",
            delta=f"Target: <{DashboardConfig.GATING_SENSITIVITY_THRESHOLD}",
            delta_color=delta_color
        )
    
    with col3:
        avg_winner_flip = metrics.get('avg_winner_flip_rate', 0)
        delta_color = "inverse" if avg_winner_flip > DashboardConfig.WINNER_FLIP_THRESHOLD else "normal"
        st.metric(
            label="Avg Winner Flip Rate",
            value=f"{avg_winner_flip:.4f}",
            delta=f"Target: <{DashboardConfig.WINNER_FLIP_THRESHOLD}",
            delta_color=delta_color
        )
    
    with col4:
        avg_latency = metrics.get('avg_latency_ms', 0)
        delta_color = "inverse" if avg_latency > DashboardConfig.LATENCY_THRESHOLD_MS else "normal"
        st.metric(
            label="Avg Latency (ms)",
            value=f"{avg_latency:.1f}",
            delta=f"Target: <{DashboardConfig.LATENCY_THRESHOLD_MS}ms",
            delta_color=delta_color
        )

def create_stability_charts(recent_data: Dict) -> None:
    """Create stability monitoring charts"""
    if not recent_data or not recent_data.get('metrics'):
        st.warning("No recent metrics available")
        return
    
    metrics_list = recent_data['metrics']
    df = pd.DataFrame(metrics_list)
    
    if df.empty:
        st.warning("No metric data to display")
        return
    
    # Convert timestamp to datetime
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    
    # Create subplots
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=[
            'Gating Sensitivity Over Time',
            'Winner Flip Rate Over Time', 
            'Routing Entropy vs Clarity',
            'Boundary Distance Distribution'
        ],
        specs=[[{"secondary_y": False}, {"secondary_y": False}],
               [{"secondary_y": False}, {"secondary_y": False}]]
    )
    
    # Gating Sensitivity
    fig.add_trace(
        go.Scatter(
            x=df['timestamp'],
            y=df['gating_sensitivity'],
            name='Gating Sensitivity',
            line=dict(color='red', width=2),
            hovertemplate='%{y:.4f}<extra></extra>'
        ),
        row=1, col=1
    )
    fig.add_hline(
        y=DashboardConfig.GATING_SENSITIVITY_THRESHOLD,
        line_dash="dash", line_color="red",
        row=1, col=1
    )
    
    # Winner Flip Rate
    fig.add_trace(
        go.Scatter(
            x=df['timestamp'],
            y=df['winner_flip_rate'],
            name='Winner Flip Rate',
            line=dict(color='orange', width=2),
            hovertemplate='%{y:.4f}<extra></extra>'
        ),
        row=1, col=2
    )
    fig.add_hline(
        y=DashboardConfig.WINNER_FLIP_THRESHOLD,
        line_dash="dash", line_color="orange",
        row=1, col=2
    )
    
    # Entropy vs Clarity scatter
    fig.add_trace(
        go.Scatter(
            x=df['clarity_score'],
            y=df['routing_entropy'],
            mode='markers',
            name='Entropy vs Clarity',
            marker=dict(
                color=df['gating_sensitivity'],
                colorscale='Viridis',
                showscale=True,
                size=6,
                colorbar=dict(title="Gating Sensitivity")
            ),
            hovertemplate='Clarity: %{x:.3f}<br>Entropy: %{y:.3f}<extra></extra>'
        ),
        row=2, col=1
    )
    
    # Boundary Distance Distribution
    fig.add_trace(
        go.Histogram(
            x=df['boundary_distance'],
            name='Boundary Distance',
            nbinsx=30,
            marker_color='lightblue',
            hovertemplate='Distance: %{x:.3f}<br>Count: %{y}<extra></extra>'
        ),
        row=2, col=2
    )
    
    fig.update_layout(
        height=600,
        title_text="MoE Routing Stability Metrics (4.72x Improvement)",
        showlegend=False
    )
    
    st.plotly_chart(fig, use_container_width=True)

def create_performance_summary(metrics: Dict, recent_data: Dict) -> None:
    """Create performance summary section"""
    st.subheader("üìä Performance Summary")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### Controller Statistics")
        stats_data = {
            "Total Requests": metrics.get('requests_processed', 0),
            "Uptime": metrics.get('uptime_seconds', 0),
            "Error Rate": f"{metrics.get('error_rate', 0)*100:.2f}%",
            "Avg Processing Time": f"{metrics.get('avg_latency_ms', 0):.1f}ms"
        }
        
        for key, value in stats_data.items():
            st.text(f"{key}: {value}")
    
    with col2:
        st.markdown("### Stability Metrics")
        if recent_data and recent_data.get('metrics'):
            df = pd.DataFrame(recent_data['metrics'])
            
            stability_stats = {
                "Median Gating Sensitivity": f"{df['gating_sensitivity'].median():.4f}",
                "P95 Winner Flip Rate": f"{df['winner_flip_rate'].quantile(0.95):.4f}",
                "Avg Clarity Score": f"{df['clarity_score'].mean():.3f}",
                "Min Boundary Distance": f"{df['boundary_distance'].min():.3f}"
            }
            
            for key, value in stability_stats.items():
                st.text(f"{key}: {value}")

def main():
    """Main dashboard application"""
    st.title("üéØ MoE Routing Stability Dashboard")
    st.markdown("Real-time monitoring for production MoE routing with **4.72√ó stability improvement**")
    
    # Sidebar controls
    st.sidebar.title("Controls")
    
    # Auto-refresh toggle
    auto_refresh = st.sidebar.checkbox("Auto Refresh", value=True)
    if auto_refresh:
        refresh_rate = st.sidebar.slider("Refresh Rate (seconds)", 1, 30, DashboardConfig.REFRESH_INTERVAL)
    
    # Manual refresh button
    if st.sidebar.button("üîÑ Refresh Now"):
        st.cache_data.clear()
    
    # Validation test button
    if st.sidebar.button("üß™ Run Validation Test"):
        with st.spinner("Running validation test..."):
            validation_result = validate_improvement()
            if validation_result:
                status = validation_result.get('status', 'UNKNOWN')
                message = validation_result.get('message', '')
                
                if status == 'PASSED':
                    st.sidebar.success(f"‚úÖ Validation PASSED: {message}")
                else:
                    st.sidebar.error(f"‚ùå Validation FAILED: {message}")
            else:
                st.sidebar.error("‚ùå Validation test failed to run")
    
    # Fetch current data
    health_data = fetch_health_status()
    metrics_data = fetch_metrics()
    recent_data = fetch_recent_metrics(DashboardConfig.METRICS_HISTORY_LIMIT)
    
    # Connection status
    if health_data is None:
        st.error("üî¥ Cannot connect to MoE Routing API. Please ensure the server is running on http://localhost:8000")
        st.stop()
    
    # Status indicator
    create_status_indicator(
        health_data.get('status', 'unknown'),
        health_data.get('improvement_factor', 'N/A')
    )
    
    # Metrics cards
    if metrics_data:
        create_metrics_cards(metrics_data)
    
    st.markdown("---")
    
    # Stability charts
    st.subheader("üìà Real-Time Stability Monitoring")
    create_stability_charts(recent_data)
    
    st.markdown("---")
    
    # Performance summary
    if metrics_data:
        create_performance_summary(metrics_data, recent_data)
    
    # Raw data expander
    with st.expander("üîç Raw Data (Debug)"):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.subheader("Health Data")
            st.json(health_data)
        
        with col2:
            st.subheader("Metrics Data") 
            st.json(metrics_data)
        
        with col3:
            st.subheader("Recent Metrics Count")
            if recent_data:
                st.text(f"Records: {recent_data.get('count', 0)}")
    
    # Auto-refresh
    if auto_refresh:
        time.sleep(refresh_rate)
        st.rerun()

if __name__ == "__main__":
    main()